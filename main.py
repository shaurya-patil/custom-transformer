import numpy as np
from config import TransformerConfig
from transformer import Transformer
from utils import tokenize, detokenize, create_padding_mask, create_look_ahead_mask

def main():
    # 1. Configuration
    print("Initializing Transformer Configuration...")
    config = TransformerConfig(
        vocab_size=260,  # 256 bytes + specials
        d_model=128,     # Smaller for demo
        n_layers=2,
        n_heads=4,
        d_ff=512,
        max_seq_len=100
    )
    
    # 2. Model Initialization
    print("Initializing Transformer Model...")
    model = Transformer(config)
    
    # 3. Dummy Data
    input_text = "Hello World"
    target_text = "Hello Byte"
    
    print(f"Input Text: {input_text}")
    print(f"Target Text: {target_text}")
    
    # Tokenize
    inp_ids = tokenize(input_text)
    tar_ids = tokenize(target_text)
    
    # Add batch dimension
    inp = np.array([inp_ids])  # (1, seq_len)
    tar = np.array([tar_ids])  # (1, seq_len)
    
    print(f"Input Shape: {inp.shape}")
    print(f"Target Shape: {tar.shape}")
    
    # 4. Create Masks
    # Encoder padding mask (no padding in this simple example, so all 0s for mask logic, but wait... 
    # create_padding_mask returns 1 for padding. If no padding, all 0s.)
    enc_padding_mask = create_padding_mask(inp)
    
    # Decoder look-ahead mask (for self-attention)
    look_ahead_mask = create_look_ahead_mask(tar.shape[1])
    dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = np.maximum(dec_target_padding_mask, look_ahead_mask)
    
    # Decoder padding mask (for cross-attention)
    dec_padding_mask = create_padding_mask(inp)
    
    # 5. Forward Pass
    print("Running Forward Pass...")
    # Note: In training, we feed tar[:, :-1] as input and tar[:, 1:] as real target.
    # Here we just pass the whole sequence to verify shapes.
    
    final_output, attention_weights = model.forward(
        inp, 
        tar, 
        enc_padding_mask, 
        combined_mask, 
        dec_padding_mask
    )
    
    print("Forward Pass Complete!")
    print(f"Output Shape: {final_output.shape}")
    
    expected_shape = (1, tar.shape[1], config.vocab_size)
    assert final_output.shape == expected_shape, f"Shape mismatch! Expected {expected_shape}, got {final_output.shape}"
    print("Shape Verification Passed!")
    
    # 6. Basic Generation Loop (Greedy Decode)
    print("\n--- Generation Demo (Random Weights) ---")
    start_token = tokenize("H")[0]
    generated = [start_token]
    
    # We want to generate "Hello" -> expecting random garbage but checking loop works
    for _ in range(5):
        curr_tar = np.array([generated])
        
        # Masks
        look_ahead = create_look_ahead_mask(curr_tar.shape[1])
        dec_target_pad = create_padding_mask(curr_tar)
        combined = np.maximum(dec_target_pad, look_ahead)
        
        output, _ = model.forward(
            inp,
            curr_tar,
            enc_padding_mask,
            combined,
            dec_padding_mask
        )
        
        # Get last token predictions
        predictions = output[:, -1, :]  # (batch, vocab_size)
        predicted_id = np.argmax(predictions, axis=-1)[0]
        
        generated.append(predicted_id)
        
    print(f"Generated Bytes: {generated}")
    print(f"Detokenized: {detokenize(generated)}")

if __name__ == "__main__":
    main()
